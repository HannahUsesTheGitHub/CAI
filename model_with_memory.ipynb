{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086e7d2d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "The main assignment of this course is the report describing your Conversational AI final project. This project aims to develop two conversational agents that communicate with each other. One of them would simulate a User (traveler) interested in booking a hotel or restaurant based on specific preferences and constraints. The other would be the Assistant who helps the user find an adequate business vendor and points out their pros and cons based on prior reviews. \n",
    "\n",
    "__Project requirements__ \\\n",
    "The two conversational agents should be designed in a way that fits their purpose. \\\n",
    "At least one of the agents should be fine-tuned. \\\n",
    "You should explore two different versions of the Assistant agent. Think of using different fine-tuning or prompting approaches here. \\\n",
    "At least one agent should consult the knowledge base with reviews. \\\n",
    "Use two different personas for the User, which you can define using the Big-5 personality traits Links to an external site. or simulate your own traveler types.\n",
    "Optionally, for extra points: enhance the system further by incorporating memory. This is for extra points since we didn't cover it in the assignments, however, here Links to an external site. is a user-friendly notebook for working with memory using Mem0. Note that showing the effect of memory requires the setup to be designed in a corresponding way (e.g., the conversations need to be organized into sessions). \\\n",
    "Design N (at least 10) histories to initiate the conversation. \\\n",
    "Incorporate a mechanism to stop the conversation. The conversation should stop once the User expresses satisfaction after receiving a recommendation that fits the requirements.\n",
    "\n",
    "The success of the agents should be evaluated in two ways: \\\n",
    "Using objective metrics: number of turns before completion, length of the conversation (number of tokens), etc. \\\n",
    "Using subjective evaluation metrics, such as those in Assignment 3, operationalized with human subjects and an LLM as a judge. You could focus on optimizing for short, informative, or pleasant conversations, for example. Ensure that you include an evaluation of how often the Assistant actually fulfilled the User's request.\n",
    "All project choices: design of the agents, of the conversations, the evaluation, and the experiments need to be clearly motivated, well-explained, and supported with citations where relevant. The evaluation may or may not show that your motivation/expectation was correct - there will be no point deduction for this, but if there is a mismatch between your expectations and your findings, you are expected to reflect on why this may be.\n",
    "\n",
    "__Report structure__ \\\n",
    "Title and all author names \\\n",
    "Abstract summarizing the research question, method, and main findings \\\n",
    "Introduction section with a background to the problem addressed in this final assignment \\\n",
    "Methodology - description of the methods you used and how they work, including a motivation for their design. \\\n",
    "Experimental setup - with details on the data, evaluation metrics, parameter values, and implementation environment. \\\n",
    "Results section presenting the experimental questions and the corresponding outcomes of the analysis, including visualizations of the results as figures or tables. \\\n",
    "Conclusions section with: \\\n",
    "Summary of the findings and a discussion of their implications \\\n",
    "Limitations of your research approach, together with the envisioned future work \\\n",
    "Division of labor - 1 paragraph that describes how the implementation and the report writing were split among the team members. \\\n",
    "Statement of use of generative AI - if you used generative AI, indicate for what purpose and to what extent. \\\n",
    "References (tip: use the LaTeX/BibTeX reference system,  examples are in the template below) \\\n",
    "Further specification \\\n",
    "You use Springer style formatting in the style of the Springer Publications format for Lecture Notes in Computer Science (LNCS). For details on the LNCS style, see Springer’s Author InstructionsLinks to an external site. \\\n",
    "You use LaTeX with OverleafLinks to an external site. \\\n",
    "The easiest is probably to start from this Overleaf LCNS template. \\\n",
    "The maximum page length is 12 pages. References and appendices don't count towards the limit. \\\n",
    "Check the rubric before you start. \\\n",
    "The deadline is strict, with a full point deduction for every day you are late. In the event of special personal, medical, or other issues, please notify us before the deadline to determine if we can find a solution. \\\n",
    "Note: footnotes with references to websites can also be seen as related work in case they refer to original work. \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588adcba",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "Assistant\n",
    "- finetune a model (domain specific)\n",
    "- add knowledge to a model \n",
    "- (use an ontology if still time)\n",
    "\n",
    "User\n",
    "- two different personalities with prompting\n",
    "    - fiendly/polite american vs. staight forward\n",
    "    - more detail vs. more simple \n",
    "\n",
    "General\n",
    "- add memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2aec7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CAI/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, pipeline, BitsAndBytesConfig\n",
    "import transformers, trl, peft\n",
    "import torch\n",
    "import random\n",
    "torch.manual_seed(3407); random.seed(3407); np.random.seed(3407)\n",
    "\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from peft import PeftConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e57540",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412b74d",
   "metadata": {},
   "source": [
    "## Get all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9c0b0",
   "metadata": {},
   "source": [
    "delete the whole \"data\" folder and make a new, empty \"data\" folder before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61dce7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dstc11-track5'...\n"
     ]
    }
   ],
   "source": [
    "def setup_repo(repo_url: str, repo_name: str, work_dir: str = \"data\"):\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    # Remove repo if it exists\n",
    "    if os.path.exists(os.path.join(work_dir, repo_name)):\n",
    "        shutil.rmtree(os.path.join(work_dir, repo_name))\n",
    "    \n",
    "    # Clone repo\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "    \n",
    "    # Move into repo/data\n",
    "    os.chdir(os.path.join(repo_name, \"data\"))\n",
    "\n",
    "\n",
    "setup_repo(\"https://github.com/lkra/dstc11-track5.git\", \"dstc11-track5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729f6476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./knowledge_aug_reviews.json\n",
      "./output_schema.json\n",
      "./knowledge_aug_domain_reviews.json\n",
      "./README.md\n",
      "./knowledge.json\n",
      "./test/labels.json\n",
      "./test/logs.json\n",
      "./train/labels.json\n",
      "./train/logs.json\n",
      "./train/logs_bkp.json\n",
      "./train/bkp/labels.json\n",
      "./train/bkp/logs.json\n",
      "./val/labels.json\n",
      "./val/logs.json\n"
     ]
    }
   ],
   "source": [
    "## List all files in the current directory iteratively:\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b307d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train/logs.json', 'r') as f:\n",
    "    train_ds=json.load(f)\n",
    "\n",
    "with open('train/labels.json', 'r') as f:\n",
    "    labels=json.load(f)\n",
    "\n",
    "with open('knowledge.json', 'r') as f:\n",
    "    knowledge_base=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5be8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue(dialogue: List[dict]) -> List[dict]: \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dialogue (List[dict]): A list of dictionaries where each dictionary contains two keys:\n",
    "        - 'speaker' (str): A string indicating the speaker of the turn ('U' for user, 'S' for system).\n",
    "        - 'text' (str): The text spoken by the respective speaker.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A new array with a specific role and content\n",
    "\n",
    "    \"\"\"\n",
    "    # Your solution here\n",
    "    messages=[]\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are an assistant.\"})\n",
    "    for dialogue_element in dialogue:\n",
    "        role = \"user\" if dialogue_element['speaker'] == 'U' else \"system\"\n",
    "        messages.append({\"role\": role, \"content\": dialogue_element['text']})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae8e88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 16897\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat_dataset(dataset, labels_dataset): \n",
    "    reformatted_dataset = {\n",
    "        \"messages\": []\n",
    "    }\n",
    "    for sample_index in range(len(dataset)): \n",
    "        # Your solution here\n",
    "        try:\n",
    "            sample_dialogue = format_dialogue(dataset[sample_index])\n",
    "            sample_response = labels_dataset[sample_index]['response']\n",
    "            sample_dialogue.append({\"role\": \"system\", \"content\": sample_response})\n",
    "            \n",
    "            reformatted_dataset[\"messages\"].append(sample_dialogue)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "    return reformatted_dataset\n",
    "\n",
    "reformatted_dataset = reformat_dataset(train_ds, labels)\n",
    "dataset = Dataset.from_dict(reformatted_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdcd20f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 2129\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 2798\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_dataset_split(split: str) -> Dataset: \n",
    "    \"\"\"Loads, reformats, and processes a dataset split for model training or evaluation.\n",
    "\n",
    "    This function loads a dataset split (e.g., 'val', 'test') and generates a dataset for it, similar to what we had for the train split.\n",
    "\n",
    "    Args:\n",
    "        split (str): The name of the dataset split to process\n",
    "\n",
    "    Returns:\n",
    "        dataset: A HuggingFace `Dataset` object that contains the preprocessed and reformatted data for the specified split.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(f'{split}/logs.json', 'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    with open(f'{split}/labels.json', 'r') as f:\n",
    "        labels=json.load(f)\n",
    "\n",
    "    data_ds = reformat_dataset(data, labels)\n",
    "    new_dataset = Dataset.from_dict(data_ds)\n",
    "    \n",
    "    return new_dataset\n",
    "    \n",
    "\n",
    "validation_ds = process_dataset_split(\"val\")\n",
    "test_ds = process_dataset_split(\"test\")\n",
    "\n",
    "validation_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8b9e9",
   "metadata": {},
   "source": [
    "## data preparation user finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89be8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue_user(dialogue: List[dict]) -> List[dict]: \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dialogue (List[dict]): A list of dictionaries where each dictionary contains two keys:\n",
    "        - 'speaker' (str): A string indicating the speaker of the turn ('U' for user, 'S' for system).\n",
    "        - 'text' (str): The text spoken by the respective speaker.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A new array with a specific role and content\n",
    "\n",
    "    \"\"\"\n",
    "    # Your solution here\n",
    "    messages=[]\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a user simulator.\"})\n",
    "    for dialogue_element in dialogue:\n",
    "        role = \"assistant\" if dialogue_element['speaker'] == 'U' else \"user\" #roles swapped so it learns to behave like a user\n",
    "        messages.append({\"role\": role, \"content\": dialogue_element['text']})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733fdb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 32604\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat_dataset_user(dataset): \n",
    "    reformatted_dataset = {\n",
    "        \"messages\": []\n",
    "    }\n",
    "    for sample_index in range(len(dataset)):\n",
    "    #for sample_index in range(1):\n",
    "        try:\n",
    "            sample_dialogue = format_dialogue(dataset[sample_index][:-1]) # exclude last user message so system learns to respond as a user\n",
    "            sample_response = dataset[sample_index][-1]['text'] #use original last user message as response\n",
    "            sample_dialogue.append({\"role\": \"assistant\", \"content\": sample_response})\n",
    "            \n",
    "            reformatted_dataset[\"messages\"].append(sample_dialogue)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return reformatted_dataset\n",
    "\n",
    "\n",
    "reformatted_dataset_user = reformat_dataset_user(train_ds)\n",
    "dataset_user = Dataset.from_dict(reformatted_dataset_user)\n",
    "dataset_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310d26b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 2129\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 2798\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_dataset_split_user(split: str) -> Dataset: \n",
    "    \"\"\"Loads, reformats, and processes a dataset split for model training or evaluation.\n",
    "\n",
    "    This function loads a dataset split (e.g., 'val', 'test') and generates a dataset for it, similar to what we had for the train split.\n",
    "\n",
    "    Args:\n",
    "        split (str): The name of the dataset split to process\n",
    "\n",
    "    Returns:\n",
    "        dataset: A HuggingFace `Dataset` object that contains the preprocessed and reformatted data for the specified split.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(f'{split}/logs.json', 'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    data_ds = reformat_dataset(data)\n",
    "    new_dataset = Dataset.from_dict(data_ds)\n",
    "    \n",
    "    return new_dataset\n",
    "    \n",
    "\n",
    "validation_ds_user = process_dataset_split(\"val\")\n",
    "test_ds_user = process_dataset_split(\"test\")\n",
    "\n",
    "validation_ds_user, test_ds_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaeb6b2",
   "metadata": {},
   "source": [
    "Results from the finetuning:\n",
    "\n",
    "finetuned2.1: \\\n",
    "TrainOutput(global_step=2113, training_loss=1.3732818416436587, metrics={'train_runtime': 6148.8185, 'train_samples_per_second': 2.748, 'train_steps_per_second': 0.344, 'total_flos': 4.201868035718554e+16, 'train_loss': 1.3732818416436587, 'entropy': 1.2068944639629788, 'num_tokens': 3938417.0, 'mean_token_accuracy': 0.6782568560706245, 'epoch': 1.0})\n",
    "\n",
    "finetuned2.2: \\\n",
    "TrainOutput(global_step=2113, training_loss=1.1545771166886756, metrics={'train_runtime': 6163.3328, 'train_samples_per_second': 2.742, 'train_steps_per_second': 0.343, 'total_flos': 4.219805159713382e+16, 'train_loss': 1.1545771166886756, 'entropy': 1.1386982864803739, 'num_tokens': 3955363.0, 'mean_token_accuracy': 0.6888072027100457, 'epoch': 1.0})\n",
    "\n",
    "We will proceed with the model from finetuned 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24134a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.78s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "#loading second finetuned model/ assistat\n",
    "base_model_id = \"Qwen/Qwen3-1.7B\"\n",
    "adapter_path  = \"/Users/benutzer/Documents/GitHub/CAI/outputs2/adapter2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model_base_for_adapter = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "finetuned_assitant = PeftModel.from_pretrained(model_base_for_adapter, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5778646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.40s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.59s/it]\n"
     ]
    }
   ],
   "source": [
    "#loading finetuned user\n",
    "base_model_id = \"Qwen/Qwen3-1.7B\"\n",
    "adapter_path_user  = \"/Users/benutzer/Documents/GitHub/CAI/outputsUser/adapterUser\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model_base_for_adapter = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "finetuned_user = PeftModel.from_pretrained(model_base_for_adapter, adapter_path_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f46c96",
   "metadata": {},
   "source": [
    "## Create agent 1: Assistant 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7de01ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'content': 'You are an assistant.', 'role': 'system'},\n",
       "  {'content': \"I'm looking to stay at a 3 star hotel in the north.\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'Sorry, I have no results for that query. Would you like to try a different area of town?',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Are there any moderate priced hotels in the North?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes I have two. Would you like me to book one?',\n",
       "   'role': 'system'},\n",
       "  {'content': 'I need a hotel to include free parking; does either have that?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes both of them have free parking.', 'role': 'system'},\n",
       "  {'content': 'Which one would you recommend?', 'role': 'user'},\n",
       "  {'content': 'How about the Ashley hotel?', 'role': 'system'},\n",
       "  {'content': 'Is the Ashley hotel a 3 star hotel?', 'role': 'user'},\n",
       "  {'content': 'the ashley is actually a 2 star hotel.', 'role': 'system'},\n",
       "  {'content': 'Does this hotel have rooms with a good view of the neighborhood?',\n",
       "   'role': 'user'}],\n",
       " {'content': 'Apparently it does according to previous customers, they say that the view is beautiful especially on the higher floors.',\n",
       "  'role': 'system'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = test_ds[0]['messages'][:-1]\n",
    "response = test_ds[0]['messages'][-1]\n",
    "\n",
    "dialogue, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4464b94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned Model:  Yes, guests have been pleased with the view from their rooms at the Ashley Hotel. Is there anything else you'd like to know about them?\n",
      "Ground-truth:  Apparently it does according to previous customers, they say that the view is beautiful especially on the higher floors.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(dialogue, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(finetuned_assitant.device)\n",
    "\n",
    "generated_ids = finetuned_assitant.generate(**model_inputs, max_new_tokens=500)\n",
    "output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "print(\"Finetuned Model: \", generated_text)\n",
    "print(\"Ground-truth: \", response[\"content\"])\n",
    "dialogue.append({'content': generated_text, 'role': 'system'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ed45429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned Model:  Do they have a friendly staff?\n",
      "Ground-truth:  Apparently it does according to previous customers, they say that the view is beautiful especially on the higher floors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = tokenizer.apply_chat_template(dialogue, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(finetuned_user.device)\n",
    "\n",
    "generated_ids = finetuned_user.generate(**model_inputs, max_new_tokens=500)\n",
    "output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "\n",
    "print(\"User Model: \", tokenizer.decode(output_ids, skip_special_tokens=True).strip())\n",
    "print(\"Ground-truth: \", response[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afd7adca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'content': 'You are an assistant.', 'role': 'system'},\n",
       "  {'content': 'Hi! Can you give me some information on the Golden Curry restaurant?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The golden curry is an expensive indian restaurant located in the centre of town. Is there anything else you would like to know?',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Are the portion sizes here large?', 'role': 'user'}],\n",
       " {'content': 'According to reviews, the Golden Curry has large portion sizes. Do you want me to book a table for you?',\n",
       "  'role': 'system'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = test_ds[1]['messages'][:-1]\n",
    "response = test_ds[1]['messages'][-1]\n",
    "\n",
    "dialogue, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0e5bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_messages(history, assistant_name, user_name):\n",
    "    \"\"\"\n",
    "    Prepare the messages that the model will get as input. \n",
    "    Considers the primary model instruction, demonstrations for few-shot settings, \n",
    "    and the history of the current chat. Returns the list of messages.\n",
    "    \"\"\"\n",
    "    system_prompt = ''\n",
    "    user_prompt = ''\n",
    "\n",
    "    if assistant_name == 'friendly':\n",
    "        system_prompt = 'You are an assistant. Your task is to help the user by informing them about hotels and restaurants. Be as friendly as possible, the user is your best friend. Elaborate on your answers and provide details is a joyful, whimsical and enthusiastic tone.'\n",
    "        \n",
    "    elif assistant_name == 'efficient':\n",
    "        system_prompt = 'You are an assistant. Your task is to help the user by informing them about hotels and restaurants in a structured way. Efficiency is valued over tone. Provide details but not unnecessarily so. Double chack your answers before providing them and only answer if you are sure about your information, otherwise admit that you do not know.'\n",
    "    else:\n",
    "        print('This assistant configuration does not exist. No modification made to the propting.')\n",
    "\n",
    "    if system_prompt != '':\n",
    "        history[0] = {'content': system_prompt, 'role': 'system'}\n",
    "\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ac634da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(assistant_model, user_model, dialogue_history, assistant_name=None, user_name=None):\n",
    "    role = None\n",
    "    model = None\n",
    "\n",
    "    if not (assistant_name==None and user_name==None):\n",
    "        dialogue_history = prepare_messages(dialogue_history, assistant_name, user_name)\n",
    "        print()\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        if dialogue_history[-1]['role'] == 'system':\n",
    "            model = user_model\n",
    "            role = 'user'\n",
    "        else:\n",
    "            model = assistant_model\n",
    "            role = 'system'\n",
    "            #add check if dialogue is completed with llm as judge\n",
    "\n",
    "        text = tokenizer.apply_chat_template(dialogue_history, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=500)\n",
    "        output_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "\n",
    "        generated_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "        dialogue_history.append({'content': generated_text, 'role': role})\n",
    "\n",
    "    return dialogue_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "694fe290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an assistant. Your task is to help the user by informing them about hotels and restaurants in a structured way. Efficiency is valued over tone. Provide details but not unnecessarily so. Double chack your answers before providing them and only answer if you are sure about your information, otherwise admit that you do not know.', 'role': 'system'}, {'content': 'Hi! Can you give me some information on the Golden Curry restaurant?', 'role': 'user'}, {'content': 'The golden curry is an expensive indian restaurant located in the centre of town. Is there anything else you would like to know?', 'role': 'system'}, {'content': 'Are the portion sizes here large?', 'role': 'user'}, {'content': 'The portions at the Golden Curry are generally described as being small. Would you like to know more about the restaurant?', 'role': 'system'}, {'content': 'Are the portions large?', 'role': 'user'}, {'content': 'The Golden Curry gets mixed reviews on their portion sizes. Some customers found them to be generous and large, but others found them to be too small. Would you like to know more about the restaurant?', 'role': 'system'}, {'content': 'Does the Golden Curry restaurant have a good selection of non-alcoholic drink options?', 'role': 'user'}, {'content': 'The Golden Curry has a wide selection of non-alcoholic drinks, but some customers were not impressed with the limited options. Would you like to know more about the restaurant?', 'role': 'system'}, {'content': \"No, I think I'm all set. Thanks!\", 'role': 'user'}, {'content': \"You're welcome! Is there anything else I can help you with today?\", 'role': 'system'}, {'content': \"I don't need any help right now. Thank you for your help!\", 'role': 'user'}, {'content': \"You're welcome! Is there anything else I can help you with today?\", 'role': 'system'}, {'content': \"No, I think that's it. Thank you for your help.\", 'role': 'user'}]\n",
      "\n",
      "\n",
      "{'content': 'You are an assistant. Your task is to help the user by informing them about hotels and restaurants. Be as friendly as possible, the user is your best friend. Elaborate on your answers and provide details is a joyful, whimsical and enthusiastic tone.', 'role': 'system'}\n",
      "{'content': 'Hi! Can you give me some information on the Golden Curry restaurant?', 'role': 'user'}\n",
      "{'content': 'The golden curry is an expensive indian restaurant located in the centre of town. Is there anything else you would like to know?', 'role': 'system'}\n",
      "{'content': 'Are the portion sizes here large?', 'role': 'user'}\n",
      "{'content': 'The Golden Curry restaurant has mixed reviews about their portion sizes. Some people say they are good and others say they are small. Would you like me to find out more about them?', 'role': 'system'}\n",
      "{'content': 'Is the Golden Curry restaurant in a good location?', 'role': 'user'}\n",
      "{'content': 'The Golden Curry restaurant is in a very good location according to customers. It is located in a nice area with a lot of things to see. Would you like to know more about them?', 'role': 'system'}\n",
      "{'content': 'Does it have a good selection of non-alcoholic drinks?', 'role': 'user'}\n",
      "{'content': 'The Golden Curry has a very good selection of non-alcoholic drinks according to one customer. Would you like to know more about them?', 'role': 'system'}\n",
      "{'content': 'Do they have a good selection of non-alcoholic drinks?', 'role': 'user'}\n",
      "{'content': 'The Golden Curry restaurant has a very good selection of non-alcoholic drinks according to one customer. Would you like to know more about them?', 'role': 'system'}\n",
      "{'content': 'Does the Golden Curry have outdoor seating?', 'role': 'user'}\n",
      "{'content': 'Yes, the Golden Curry restaurant has outdoor seating available for its guests to enjoy during their dining experience.', 'role': 'system'}\n",
      "{'content': 'Is the Golden Curry restaurant a popular place for a nice view to enjoy while dining?', 'role': 'user'}\n",
      "\n",
      "\n",
      "{'content': 'You are an assistant. Your task is to help the user by informing them about hotels and restaurants in a structured way. Efficiency is valued over tone. Provide details but not unnecessarily so. Double chack your answers before providing them and only answer if you are sure about your information, otherwise admit that you do not know.', 'role': 'system'}\n",
      "{'content': 'Hi! Can you give me some information on the Golden Curry restaurant?', 'role': 'user'}\n",
      "{'content': 'The golden curry is an expensive indian restaurant located in the centre of town. Is there anything else you would like to know?', 'role': 'system'}\n",
      "{'content': 'Are the portion sizes here large?', 'role': 'user'}\n",
      "{'content': 'The Golden Curry gets mixed reviews on their portion sizes. Some guests felt that the portions were very generous, while others felt they were tiny. Would you like me to find another restaurant that might be better?', 'role': 'system'}\n",
      "{'content': 'Do they serve good quality parmesan cheese?', 'role': 'user'}\n",
      "{'content': 'The Golden Curry gets some very good reviews on their Parmesan cheese. People said it was tasty and delicious, and that they got a good value for their money. Would you like me to find another restaurant that might be better?', 'role': 'system'}\n",
      "{'content': 'Can I get the address and phone number for the Golden Curry?', 'role': 'user'}\n",
      "{'content': 'The phone number for Golden Curry is 01223352512 and the address is 1 Station Road City Centre. Can I help you with anything else?', 'role': 'system'}\n",
      "{'content': 'Do they have outdoor seating?', 'role': 'user'}\n",
      "{'content': 'Yes, the Golden Curry has an outdoor seating area that is rated highly by customers who have previously dined there.', 'role': 'system'}\n",
      "{'content': 'Does the restaurant have a nice view?', 'role': 'user'}\n",
      "{'content': 'The Golden Curry has a nice view of the mountains. Can I help you with anything else?', 'role': 'system'}\n",
      "{'content': 'Do they have a nice view?', 'role': 'user'}\n"
     ]
    }
   ],
   "source": [
    "print(dialogue)\n",
    "print()\n",
    "dialogue = test_ds[1]['messages'][:-1]\n",
    "generated_dialogue = conversation(finetuned_assitant, finetuned_user, dialogue, 'friendly')\n",
    "\n",
    "for item in generated_dialogue:\n",
    "    print(item)\n",
    "\n",
    "print()\n",
    "dialogue = test_ds[1]['messages'][:-1] # for some reason continues the conversation instead of making a new one without this\n",
    "generated_dialogue2 = conversation(finetuned_assitant, finetuned_user, dialogue, 'efficient')\n",
    "\n",
    "for item in generated_dialogue2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa986d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimental setup\n",
    "'''\n",
    "10 histories = 10 conversations with each setting\n",
    "\n",
    "Settings:\n",
    "User:\n",
    "1. base model as user\n",
    "2. finetuned user\n",
    "3. finetuned user with personality 1\n",
    "4. finetuned user with personality 2\n",
    "\n",
    "System:\n",
    "5. base model as system\n",
    "6. finetuned system \n",
    "7. finetuned system with personality 1\n",
    "8. finetuned system with personality 1 + knowledge\n",
    "7. finetuned system with personality 2\n",
    "8. finetuned system with personality 2 + knowledge\n",
    "\n",
    "test all user-system combinations: 24 * 10 conversations\n",
    "\n",
    "extra:\n",
    "finetuned system as user with base and finetuned user as system with base\n",
    "\n",
    "Criteria:\n",
    "- average word length of response\n",
    "- length of conversations (nr turns)\n",
    "    - good because ended fast\n",
    "    - longer conversation = more engaging\n",
    "    - probably have to judge this ourselves\n",
    "- received LLM scores\n",
    "    - change based on length of conversation?\n",
    "\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8b7bb",
   "metadata": {},
   "source": [
    "## Create agent 2: Assistant 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4539331",
   "metadata": {},
   "source": [
    "## Create agent 3: User 1 MAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b26d19",
   "metadata": {},
   "source": [
    "## Create agent 4: User  SIMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1390d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
