{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086e7d2d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "The main assignment of this course is the report describing your Conversational AI final project. This project aims to develop two conversational agents that communicate with each other. One of them would simulate a User (traveler) interested in booking a hotel or restaurant based on specific preferences and constraints. The other would be the Assistant who helps the user find an adequate business vendor and points out their pros and cons based on prior reviews. \n",
    "\n",
    "__Project requirements__ \\\n",
    "The two conversational agents should be designed in a way that fits their purpose. \\\n",
    "At least one of the agents should be fine-tuned. \\\n",
    "You should explore two different versions of the Assistant agent. Think of using different fine-tuning or prompting approaches here. \\\n",
    "At least one agent should consult the knowledge base with reviews. \\\n",
    "Use two different personas for the User, which you can define using the Big-5 personality traits Links to an external site. or simulate your own traveler types.\n",
    "Optionally, for extra points: enhance the system further by incorporating memory. This is for extra points since we didn't cover it in the assignments, however, here Links to an external site. is a user-friendly notebook for working with memory using Mem0. Note that showing the effect of memory requires the setup to be designed in a corresponding way (e.g., the conversations need to be organized into sessions). \\\n",
    "Design N (at least 10) histories to initiate the conversation. \\\n",
    "Incorporate a mechanism to stop the conversation. The conversation should stop once the User expresses satisfaction after receiving a recommendation that fits the requirements.\n",
    "\n",
    "The success of the agents should be evaluated in two ways: \\\n",
    "Using objective metrics: number of turns before completion, length of the conversation (number of tokens), etc. \\\n",
    "Using subjective evaluation metrics, such as those in Assignment 3, operationalized with human subjects and an LLM as a judge. You could focus on optimizing for short, informative, or pleasant conversations, for example. Ensure that you include an evaluation of how often the Assistant actually fulfilled the User's request.\n",
    "All project choices: design of the agents, of the conversations, the evaluation, and the experiments need to be clearly motivated, well-explained, and supported with citations where relevant. The evaluation may or may not show that your motivation/expectation was correct - there will be no point deduction for this, but if there is a mismatch between your expectations and your findings, you are expected to reflect on why this may be.\n",
    "\n",
    "__Report structure__ \\\n",
    "Title and all author names \\\n",
    "Abstract summarizing the research question, method, and main findings \\\n",
    "Introduction section with a background to the problem addressed in this final assignment \\\n",
    "Methodology - description of the methods you used and how they work, including a motivation for their design. \\\n",
    "Experimental setup - with details on the data, evaluation metrics, parameter values, and implementation environment. \\\n",
    "Results section presenting the experimental questions and the corresponding outcomes of the analysis, including visualizations of the results as figures or tables. \\\n",
    "Conclusions section with: \\\n",
    "Summary of the findings and a discussion of their implications \\\n",
    "Limitations of your research approach, together with the envisioned future work \\\n",
    "Division of labor - 1 paragraph that describes how the implementation and the report writing were split among the team members. \\\n",
    "Statement of use of generative AI - if you used generative AI, indicate for what purpose and to what extent. \\\n",
    "References (tip: use the LaTeX/BibTeX reference system,  examples are in the template below) \\\n",
    "Further specification \\\n",
    "You use Springer style formatting in the style of the Springer Publications format for Lecture Notes in Computer Science (LNCS). For details on the LNCS style, see Springer’s Author InstructionsLinks to an external site. \\\n",
    "You use LaTeX with OverleafLinks to an external site. \\\n",
    "The easiest is probably to start from this Overleaf LCNS template. \\\n",
    "The maximum page length is 12 pages. References and appendices don't count towards the limit. \\\n",
    "Check the rubric before you start. \\\n",
    "The deadline is strict, with a full point deduction for every day you are late. In the event of special personal, medical, or other issues, please notify us before the deadline to determine if we can find a solution. \\\n",
    "Note: footnotes with references to websites can also be seen as related work in case they refer to original work. \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588adcba",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "Assistant\n",
    "- finetune a model (domain specific)\n",
    "- add knowledge to a model \n",
    "- (use an ontology if still time)\n",
    "\n",
    "User\n",
    "- two different personalities with prompting\n",
    "    - fiendly/polite american vs. staight forward\n",
    "    - more detail vs. more simple \n",
    "\n",
    "General\n",
    "- add memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2aec7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CAI/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import transformers, trl, peft\n",
    "import torch\n",
    "import random\n",
    "torch.manual_seed(3407); random.seed(3407); np.random.seed(3407)\n",
    "\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e57540",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412b74d",
   "metadata": {},
   "source": [
    "## Get all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61dce7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dstc11-track5'...\n"
     ]
    }
   ],
   "source": [
    "def setup_repo(repo_url: str, repo_name: str, work_dir: str = \"data\"):\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    # Remove repo if it exists\n",
    "    if os.path.exists(os.path.join(work_dir, repo_name)):\n",
    "        shutil.rmtree(os.path.join(work_dir, repo_name))\n",
    "    \n",
    "    # Clone repo\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "    \n",
    "    # Move into repo/data\n",
    "    os.chdir(os.path.join(repo_name, \"data\"))\n",
    "\n",
    "\n",
    "setup_repo(\"https://github.com/lkra/dstc11-track5.git\", \"dstc11-track5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729f6476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./knowledge_aug_reviews.json\n",
      "./output_schema.json\n",
      "./knowledge_aug_domain_reviews.json\n",
      "./README.md\n",
      "./knowledge.json\n",
      "./test/labels.json\n",
      "./test/logs.json\n",
      "./train/labels.json\n",
      "./train/logs.json\n",
      "./train/logs_bkp.json\n",
      "./train/bkp/labels.json\n",
      "./train/bkp/logs.json\n",
      "./val/labels.json\n",
      "./val/logs.json\n"
     ]
    }
   ],
   "source": [
    "## List all files in the current directory iteratively:\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b307d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train/logs.json', 'r') as f:\n",
    "    train_ds=json.load(f)\n",
    "\n",
    "with open('train/labels.json', 'r') as f:\n",
    "    labels=json.load(f)\n",
    "\n",
    "with open('knowledge.json', 'r') as f:\n",
    "    knowledge_base=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5be8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue(dialogue: List[dict]) -> List[dict]: \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dialogue (List[dict]): A list of dictionaries where each dictionary contains two keys:\n",
    "        - 'speaker' (str): A string indicating the speaker of the turn ('U' for user, 'S' for system).\n",
    "        - 'text' (str): The text spoken by the respective speaker.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A new array with a specific role and content\n",
    "\n",
    "    \"\"\"\n",
    "    # Your solution here\n",
    "    messages=[]\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are an assistant.\"})\n",
    "    for dialogue_element in dialogue:\n",
    "        role = \"user\" if dialogue_element['speaker'] == 'U' else \"system\"\n",
    "        messages.append({\"role\": role, \"content\": dialogue_element['text']})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae8e88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 16897\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat_dataset(dataset, labels_dataset): \n",
    "    reformatted_dataset = {\n",
    "        \"messages\": []\n",
    "    }\n",
    "    for sample_index in range(len(dataset)): \n",
    "        # Your solution here\n",
    "        try:\n",
    "            sample_dialogue = format_dialogue(dataset[sample_index])\n",
    "            sample_response = labels_dataset[sample_index]['response']\n",
    "            sample_dialogue.append({\"role\": \"system\", \"content\": sample_response})\n",
    "            \n",
    "            reformatted_dataset[\"messages\"].append(sample_dialogue)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "    return reformatted_dataset\n",
    "\n",
    "reformatted_dataset = reformat_dataset(train_ds, labels)\n",
    "dataset = Dataset.from_dict(reformatted_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdcd20f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 2129\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 2798\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_dataset_split(split: str) -> Dataset: \n",
    "    \"\"\"Loads, reformats, and processes a dataset split for model training or evaluation.\n",
    "\n",
    "    This function loads a dataset split (e.g., 'val', 'test') and generates a dataset for it, similar to what we had for the train split.\n",
    "\n",
    "    Args:\n",
    "        split (str): The name of the dataset split to process\n",
    "\n",
    "    Returns:\n",
    "        dataset: A HuggingFace `Dataset` object that contains the preprocessed and reformatted data for the specified split.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(f'{split}/logs.json', 'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    with open(f'{split}/labels.json', 'r') as f:\n",
    "        labels=json.load(f)\n",
    "\n",
    "    data_ds = reformat_dataset(data, labels)\n",
    "    new_dataset = Dataset.from_dict(data_ds)\n",
    "    \n",
    "    return new_dataset\n",
    "    \n",
    "\n",
    "validation_ds = process_dataset_split(\"val\")\n",
    "test_ds = process_dataset_split(\"test\")\n",
    "\n",
    "validation_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e70443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32571321",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_cfg = LoraConfig(r=16, \n",
    "           lora_alpha=32, \n",
    "           lora_dropout = 0.05, \n",
    "           bias = \"none\", \n",
    "           use_rslora = False, \n",
    "           target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
    "\n",
    "model = get_peft_model(base, peft_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d8a78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_bf16():\n",
    "    if torch.cuda.is_available():\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        return major >= 8\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e9e5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 2\n",
    "LEARNING_RATE    = 1e-4\n",
    "WARMUP_STEPS     = ((2113 * NUM_TRAIN_EPOCHS)//100) * 9\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    logging_steps=10,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    weight_decay=0.01,\n",
    "    max_length=1024,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    fp16=not pick_bf16(),\n",
    "    bf16=pick_bf16(),\n",
    "    packing=False,\n",
    "    dataset_num_proc=2,\n",
    "    report_to=\"none\",\n",
    "    seed=3407\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82f1033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 16897/16897 [00:10<00:00, 1612.10 examples/s]\n",
      "Truncating train dataset (num_proc=2): 100%|██████████| 16897/16897 [00:00<00:00, 17122.97 examples/s]\n",
      "Tokenizing eval dataset (num_proc=2): 100%|██████████| 2129/2129 [00:02<00:00, 966.67 examples/s] \n",
      "Truncating eval dataset (num_proc=2): 100%|██████████| 2129/2129 [00:00<00:00, 10632.50 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,      \n",
    "    eval_dataset=validation_ds,\n",
    "    args=sft_args,\n",
    "    processing_class=tok\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f9340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/opt/anaconda3/envs/CAI/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='4226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/4226 06:18 < 63:15:44, 0.02 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f46c96",
   "metadata": {},
   "source": [
    "## Create agent 1: Assistant 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"outputs/adapter\")  \n",
    "tok.save_pretrained(\"outputs/adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8b7bb",
   "metadata": {},
   "source": [
    "## Create agent 2: Assistant 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4539331",
   "metadata": {},
   "source": [
    "## Create agent 3: User 1 MAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b26d19",
   "metadata": {},
   "source": [
    "## Create agent 4: User  SIMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1390d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "two\n",
      "one\n",
      "two\n"
     ]
    }
   ],
   "source": [
    "mydict = {'one': 1, 'two': 2}\n",
    "for key in mydict:\n",
    "    print(key)\n",
    "\n",
    "for key in mydict.keys():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
